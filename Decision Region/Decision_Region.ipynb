{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Loading Data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [0, 3]] # sepal length and petal width\n",
    "y = iris.target\n",
    " \n",
    "# standardize\n",
    "X[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
    "X[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
    " \n",
    "lr = LogisticRegression(penalty='l2', \n",
    "                        dual=False, \n",
    "                        tol=0.000001, \n",
    "                        C=10.0,\n",
    "                        fit_intercept=True, \n",
    "                        intercept_scaling=1, \n",
    "                        class_weight=None, \n",
    "                        random_state=1, \n",
    "                        solver='newton-cg', \n",
    "                        max_iter=100, \n",
    "                        multi_class='multinomial', \n",
    "                        verbose=0, \n",
    "                        warm_start=False, \n",
    "                        n_jobs=1)\n",
    "lr.fit(X, y) \n",
    "y_pred = lr.predict(X)\n",
    "print('Last 3 Class Labels: %s' % y_pred[-3:])\n",
    "#\n",
    "#pip install mlxtend  \n",
    "#\n",
    "#from mlxtend.evaluate import plot_decision_regions\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X, y, clf=lr)\n",
    "plt.title('Softmax Regression in scikit-learn')\n",
    "plt.show()\n",
    "#\n",
    "#Example 1 - Decision regions in 2D\n",
    "#\n",
    "###############################\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training a classifier\n",
    "svm = SVC(C=0.5, kernel='linear')\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=svm, \n",
    "                      res=0.02, legend=2)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('SVM on Iris')\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Example 2 - Decision regions in 1D\n",
    "#\n",
    "###############################\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2]\n",
    "X = X[:, None]\n",
    "y = iris.target\n",
    "\n",
    "# Training a classifier\n",
    "svm = SVC(C=0.5, kernel='linear')\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=svm, \n",
    "                      res=0.02, legend=2)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.title('SVM on Iris')\n",
    "\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Example 3 - Decision Region Grids\n",
    "#\n",
    "###############################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC()\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0,2]]\n",
    "y = iris.target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Example 4 - Highlighting Test Data Points\n",
    "#\n",
    "###############################\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0,2]], iris.target\n",
    "X, y = shuffle_arrays_unison(arrays=[X, y], random_seed=3)\n",
    "\n",
    "X_train, y_train = X[:100], y[:100]\n",
    "X_test, y_test = X[100:], y[100:]\n",
    "\n",
    "# Training a classifier\n",
    "svm = SVC(C=0.5, kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=svm, res=0.02, \n",
    "                      legend=2, X_highlight=X_test)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('SVM on Iris')\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Example 5 - Evaluating Classifier Behavior on Non-Linear Problems\n",
    "#\n",
    "###############################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, \n",
    "                              random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC()\n",
    "\n",
    "# Loading Plotting Utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import numpy as np\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(300, 2)\n",
    "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
    "             dtype=int)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Half-Moons\n",
    "#\n",
    "###############################\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()\n",
    "###############################\n",
    "#\n",
    "#Concentric Circles\n",
    "#\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "\n",
    "plt.show()\n",
    "###############################\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
